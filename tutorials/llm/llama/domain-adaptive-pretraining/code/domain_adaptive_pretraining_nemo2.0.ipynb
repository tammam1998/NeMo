{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd13460c",
   "metadata": {},
   "source": [
    "# Domain Adaptive Pre-Training (DAPT)\n",
    "\n",
    "## Goal\n",
    "\n",
    "Given a foundational language model (in this case llama-2-7B) that was pre-trained on a broad, general-purpose corpus, our goal is to further pretrain the model on a specific domain (in this example, ChipDesign) to enhance its understanding of domain-specific language and context. This process is called Domain-Adaptive Pretraining (DAPT). DAPT adapts a general-purpose model to specialized tasks within a particular field. Instead of training from scratch, we aim to “specialize” the model by focusing on a target domain corpus, allowing it to adapt to the unique vocabulary, semantics, and syntax of that field.\n",
    "\n",
    "Our primary goals with respect to DAPT are as follows:\n",
    "* Improve the model’s performance and accuracy on domain-specific tasks\n",
    "* Ensure the model retains general language capabilities\n",
    "* Minimize pretraining time by leveraging existing knowledge in the model\n",
    "\n",
    "DAPT typically enhances a model’s efficacy in downstream tasks for the domain by exposing it to domain-relevant texts. This pretraining phase can result in more accurate and context-aware predictions on domain-specific data, as the model gains an understanding of field-specific terminology, abbreviations, and common phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43ef563",
   "metadata": {},
   "source": [
    "# NeMo Tools and Resources\n",
    "\n",
    "* [NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea0b51f",
   "metadata": {},
   "source": [
    "# Software Requirements\n",
    "* Access to latest NeMo Framework NGC Containers\n",
    "* This playbook has been tested on: nvcr.io/nvidia/nemo:dev. It is expected to work similarly on other environments.\n",
    "\n",
    "\n",
    "#### Launch the NeMo Framework container as follows: \n",
    "\n",
    "```\n",
    "docker run -it -p 8080:8080 -p 8088:8088 --rm --gpus '\"device=0,1\"' --ipc=host --network host -v $(pwd):/workspace nvcr.io/nvidia/nemo:dev\n",
    "```\n",
    "\n",
    "#### Launch Jupyter Notebook as follows: \n",
    "```\n",
    "jupyter notebook --allow-root --ip 0.0.0.0 --port 8088 --no-browser --NotebookApp.token=''\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137e1db",
   "metadata": {},
   "source": [
    "# Hardware Requirements\n",
    "\n",
    "* This playbook has been tested on 2xA100 80G but can be scaled to multiple GPUs as well as multiple nodes by modifying the appropriate parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ecb0d3",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "* In this playbook, we will leverage chip domain/hardware datasets from open-source GitHub repositories, wiki URLs, and academic papers. Data has been processed and curated using [NeMo Curator](https://github.com/NVIDIA/NeMo-Curator/tree/main) as shown in this [playbook](https://github.com/jvamaraju/ndc_dapt_playbook/tree/dapt_jv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba16a72b",
   "metadata": {},
   "source": [
    "# Notebook Outline\n",
    "\n",
    "* Step 1: Prepare the data for pretraining. This is a multi-step process discussed in detail later in the specific section (later in the notebook).\n",
    "\n",
    "* Step 2: Download the llama-2-7B hugging face checkpoint and convert to .nemo format.\n",
    "\n",
    "* Step 3: Continued pretraining the llama-2-7b model using the prepared data and the custom trained tokenizer (from the previous notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e8b1f",
   "metadata": {},
   "source": [
    "# Step 0: Clone the Model Checkpoint\n",
    "\n",
    "This notebook assumed the model has been cloned from [hugging face](https://huggingface.co/meta-llama/Llama-2-7b-hf) in the mounted directory ```/workspace```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc658bd",
   "metadata": {},
   "source": [
    "Clone the model: \n",
    "```\n",
    "git lfs install\n",
    "git clone https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec372453",
   "metadata": {},
   "source": [
    "# Step 1: Data Preparation for pretraining\n",
    "\n",
    "Identify the different file types (example: code, text, etc) in the pretraining data, in this case we only have 'code' type files. This is typically dataset dependent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c935b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from nemo.collections.llm import Llama2Config7B\n",
    "\n",
    "\n",
    "# Function to count the number of files in each of the different file types- code, text\n",
    "def identify_jsonl_files(data_path):\n",
    "    code_files = []\n",
    "    text_files = []\n",
    "    cnt_text = 0\n",
    "    cnt_code = 0\n",
    "    for root, _, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.jsonl'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r') as f:\n",
    "                    has_code = False\n",
    "                    has_text = False\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            json_obj = json.loads(line.strip())\n",
    "                            file_type = json_obj.get('file_type', '').lower()\n",
    "                            if file_type == 'code':\n",
    "                                has_code = True\n",
    "                            elif file_type == 'text':\n",
    "                                has_text = True\n",
    "                            if has_code and has_text:\n",
    "                                break\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "                if has_code:\n",
    "                    code_files.append(file_path)\n",
    "                    cnt_code = cnt_code + 1\n",
    "                if has_text:\n",
    "                    text_files.append(file_path)\n",
    "                    cnt_text = cnt_text + 1\n",
    "    return code_files, text_files, cnt_code, cnt_text\n",
    "\n",
    "# Modify data path to point to jsonl data source, in this case data_path='code/data/all_jsonl_data'\n",
    "data_path = '/workspace/dapt-custom-tokenization/code/data/all_jsonl_data'\n",
    "\n",
    "code_files, text_files, cnt_code, cnt_text = identify_jsonl_files(data_path)\n",
    "\n",
    "print(\"\\nNumber of Files containing 'file_type':'text':\", cnt_text)\n",
    "print(\"Number of Files containing 'file_type':'code':\", cnt_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60987ff2",
   "metadata": {},
   "source": [
    "### Merging code JSONL files into a single JSONL file for further preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f2e6f",
   "metadata": {},
   "source": [
    "This is an optional step, it is possible to use multiple jsonl files in this workflow as well. This example uses a single merged. jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def list_jsonl_files(directory):\n",
    "    jsonl_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.jsonl'):\n",
    "                jsonl_files.append(os.path.join(root, file))\n",
    "    return jsonl_files\n",
    "\n",
    "# Function to merge multiple jsonl files into a single file \n",
    "def merge_jsonl_files(directory, output_file):\n",
    "    jsonl_files = list_jsonl_files(directory)\n",
    "    \n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for input_file in jsonl_files:\n",
    "            with open(input_file, 'r') as infile:\n",
    "                for line in infile:\n",
    "                    try:\n",
    "                        json_object = json.loads(line.strip())\n",
    "                        json.dump(json_object, outfile)\n",
    "                        outfile.write('\\n')\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Skipping invalid JSON in {input_file}: {line.strip()}\")\n",
    "\n",
    "    print(f\"Merged {len(jsonl_files)} JSONL files into {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb0c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/workspace/dapt-custom-tokenization/code/data/all_jsonl_data'\n",
    "output_file = '/workspace/dapt-custom-tokenization/code_merged_output.jsonl'\n",
    "merge_jsonl_files(directory, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d00ad63",
   "metadata": {},
   "source": [
    "### Data Format Conversion for pretraining: JSONL to bin/idx files \n",
    "\n",
    "For efficient pretraining, we convert data from JSONL to bin/idx format. \n",
    "\n",
    "JSONL files, while convenient for storing structured text data, are not optimized for high-speed data loading during large language model training. In pretraining workflows, particularly those with large datasets and complex model architectures, the need for fast data access and efficient memory management is essential.\n",
    "\n",
    "The bin/idx format is a binary format specifically designed to facilitate high-throughput data loading. This format allows direct, randomized access to data samples, which speeds up I/O operations and reduces the memory footprint compared to loading JSONL files. By converting data to bin/idx format, hardware utilization can be maximized and bottlenecks in data processing can be avoided, leading to a more efficient pretraining process.\n",
    "\n",
    "#### Benefits of bin/idx format for Pretraining:\n",
    "\n",
    "* **Optimized I/O Performance:** The binary format enables quicker data reads and reduces latency, allowing the model to continuously access data at high speeds.\n",
    "* **Efficient Memory Usage:** Data in bin/idx format consumes less memory during loading, making it suitable for large datasets and enabling better use of available system resources.\n",
    "* **Enhanced Scalability:** With bin/idx, it’s easier to handle shuffling and batching of large datasets, which is essential for pretraining on diverse domain-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /workspace/dapt-custom-tokenization/code/code/models/tokenizer/llama2/custom_tokenizer_init_20000.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de696d7b",
   "metadata": {},
   "source": [
    "Modify the `input` to point to the merged `jsonl` file. Similarly modify paths to `vocab`, `tokenizer-model`, `merge-file` to point to relevant file paths. `tokenizer-model` should point to the custom tokenizer (trained in the custom tokenizer training notebook) if your data has domain specific terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Uncomment to use custom trained tokenizer ####\n",
    "# !python3 /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\\n",
    "# --input='/workspace/dapt-custom-tokenization/code_merged_output.jsonl' \\\n",
    "# --json-keys=text \\\n",
    "# --tokenizer-library=sentencepiece \\\n",
    "# --vocab '/workspace/dapt-custom-tokenization/code/code/models/tokenizer/llama2/custom_tokenizer_init_20000.json/vocab.json' \\\n",
    "# --dataset-impl mmap \\\n",
    "# --tokenizer-model '/workspace/Llama-2-7b-hf/tokenizer.model' \\\n",
    "# --tokenizer-type llama \\\n",
    "# --merge-file '/workspace/dapt-custom-tokenization/code/code/models/tokenizer/llama2/custom_tokenizer_init_20000.json/merges.txt' \\\n",
    "# --append-eod \\\n",
    "# --output-prefix='preprocessed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9583d-1dac-4717-b028-c78d0d703f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using default Llama-2 tokenizer for testing purpose\n",
    "!python3 /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\\n",
    "--input='/workspace/dapt-custom-tokenization/code_merged_output.jsonl' \\\n",
    "--json-keys=text \\\n",
    "--tokenizer-library=sentencepiece \\\n",
    "--dataset-impl mmap \\\n",
    "--tokenizer-model '/workspace/Llama-2-7b-hf/tokenizer.model' \\\n",
    "--tokenizer-type llama \\\n",
    "--append-eod \\\n",
    "--output-prefix='preprocessed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05efa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If the above step runs successfully, two files with the extensions .bin and .idx will be generated\n",
    "!ls "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f95149",
   "metadata": {},
   "source": [
    "# Step 2: Download and Import Llama-2-7b Hugging Face checkpoint\n",
    "\n",
    "Llama2-7B model can be automatically downloaded and converted to NeMo2 format with the following script:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile convert2nemo2.py\n",
    "from nemo.collections import llm\n",
    "from nemo.collections.llm import Llama2Config7B\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output = llm.import_ckpt(\n",
    "        model=llm.LlamaModel(config=Llama2Config7B()),\n",
    "        source=\"hf:///workspace/Llama-2-7b-hf\",\n",
    "    )"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!torchrun convert2nemo2.py",
   "id": "52ce49cce83ae6c"
  },
  {
   "cell_type": "markdown",
   "id": "b94e774b",
   "metadata": {},
   "source": [
    "The conversion will generate a ```llama-2``` NeMo2 checkpoint directory which can be used for the continued pretraining using NeMo Toolkit as shown in Step 3 in default ```$NEMO_HOME``` folder, unless otherwise specified ```NEMO_HOME``` is set as ```/root/.cache/nemo```\n",
    "\n",
    "Alternatively, you can directly use ```source=\"meta-llama/Llama2-2-7b-hf\"``` to use the model directly from Hugging Face instead of using the locally downloaded version in ```\\workspace```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c689e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1bdfe0",
   "metadata": {},
   "source": [
    "# Step 3: Continued Pretraining using Llama2-7b with NeMo2\n",
    "\n",
    "For this step we use a predefined recipe `llama2_7b.pretrain_recipe` from NeMo Toolkit for continued pretraining. We will modify the `pretrain_recipe` and use it for continued pretraining workflow. Typically this involves changing dataset files and data blends, changing learning rate scheduler, changing default parallelism based on number of devices available, adding connector to resume training, etc.\n",
    "\n",
    "First, we define the recipe and executor for using NeMo2 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a40f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo_run as run\n",
    "from nemo.collections import llm\n",
    "\n",
    "# Configure recipe to pre-train based on the default llama-2-7b recipe\n",
    "def configure_recipe(nodes: int = 1, gpus_per_node: int = 1):\n",
    "    recipe = llm.llama2_7b.pretrain_recipe(\n",
    "        name=\"llama2_7b_dapt\",\n",
    "        # Modify based on number of nodes available\n",
    "        num_nodes=nodes,\n",
    "        num_gpus_per_node=gpus_per_node,\n",
    "    )\n",
    "    # Modify\n",
    "    recipe.trainer.strategy.context_parallel_size = 1\n",
    "    recipe.trainer.strategy.tensor_model_parallel_size=1\n",
    "    recipe.trainer.val_check_interval = 100\n",
    "    return recipe\n",
    "\n",
    "# Executor for running pretraining \n",
    "def local_executor_torchrun(devices: int = 1) -> run.LocalExecutor:\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\")\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d303fc973333d",
   "metadata": {},
   "source": [
    "Let's instantiate the `recipe` and modify it so that it uses the desired number of GPUs, resuming from the pretrained Llama2-7b checkpoint instead of training from scratch.\n",
    "\n",
    "The default `recipe` initializes all the essential components required for Llama2 7B pretraining, including model, dataloader, trainer, logger, optimizer etc. `recipe` is not executed during instantiation, so it is very simple to modify it to fit your custom training workflow. In our case, we want to do the DAPT (instead of pretraining from scratch), and all we need to do is to add a `resume` config which points to the Llama2 7B checkpoint.\n",
    "\n",
    "You can easily change the optimizer, parallelism, data as per your use case. Look at the following example for guidance on how to tweak these parameters. Note: you are only configuring your task at this stage; the underlying code is not executed unless you launch the job using the executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70481ad7579de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.lightning as nl\n",
    "from nemo.collections.common.tokenizers import AutoTokenizer\n",
    "\n",
    "# Instantiate data\n",
    "data = run.Config(\n",
    "        llm.PreTrainingDataModule,\n",
    "        # Pass the path to your data here\n",
    "        paths=['preprocessed_data_text_document'],\n",
    "        seq_length=4096,\n",
    "        tokenizer=run.Config(\n",
    "            AutoTokenizer,\n",
    "            pretrained_model_name=\"/workspace/Llama-2-7b-hf\",\n",
    "        ),\n",
    "        micro_batch_size=1,\n",
    "        global_batch_size=8,\n",
    "    )\n",
    "\n",
    "\n",
    "# Instantiate the recipe\n",
    "recipe = configure_recipe(nodes=1, gpus_per_node=2)\n",
    "\n",
    "# Modify resume connector\n",
    "resume = run.Config(\n",
    "            nl.AutoResume,\n",
    "            restore_config=run.Config(nl.RestoreConfig, path=\"/root/.cache/nemo/models/Llama-2-7b-hf\"),\n",
    "        )\n",
    "recipe.resume = resume\n",
    "recipe.data.tokenizer = run.Config(\n",
    "        AutoTokenizer,\n",
    "        pretrained_model_name=\"/workspace/Llama-2-7b-hf\"\n",
    "    )\n",
    "\n",
    "# (Optional) Modify the TP/PP/CP settings\n",
    "recipe.trainer.strategy.tensor_model_parallel_size = 2\n",
    "recipe.trainer.strategy.pipeline_model_parallel_size = 1\n",
    "recipe.trainer.strategy.context_parallel_size = 1\n",
    "\n",
    "# (Optional) Modify the batch size settings\n",
    "recipe.data.global_batch_size = 8\n",
    "recipe.data.micro_batch_size = 1\n",
    "\n",
    "# (Optional) Modify the checkpoint and log location\n",
    "recipe.log.log_dir= \"/workspace/logs_01_31\"\n",
    "\n",
    "# (Optional) Modify the learning rate scheudler\n",
    "recipe.optim.config.lr = 1e-5\n",
    "recipe.optim.lr_scheduler.min_lr = 1e-6\n",
    "\n",
    "# If not configured, the recipe uses mock data for pretraining\n",
    "recipe.data = data\n",
    "\n",
    "# (Optional) Modify the data blends\n",
    "# recipe.data.paths = [0.2, 'path/to/data1', 0.1, 'path/to/data2']\n",
    "# recipe.data.paths = [1, 'preprocessed_data_text_document']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b9f780763d641",
   "metadata": {},
   "source": [
    "After configure the training procedure properly, we can run the training by instantiate the `executor` and use `nemorun` to start the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f8b3071d8ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the pretraining job \n",
    "executor = local_executor_torchrun(devices=recipe.trainer.devices)\n",
    "run.run(recipe, executor=executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf30d8c8",
   "metadata": {},
   "source": [
    "### To monitor the training, launch Tensorboard from another terminal\n",
    "\n",
    "`tensorboard --logdir nemo_experiments --bind_all`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
